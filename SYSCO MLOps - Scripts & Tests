# ============================================================================
# File: scripts/setup_project.sh (continued)

# Create GCS buckets
echo "üóÇÔ∏è  Creating GCS buckets..."
gsutil mb -p $PROJECT_ID -c STANDARD -l $REGION gs://sysco-forecasting-bucket-$PROJECT_ID || true
gsutil mb -p $PROJECT_ID -c STANDARD -l $REGION gs://sysco-terraform-state || true

# Create BigQuery dataset
echo "üìä Creating BigQuery dataset..."
bq mk --dataset \
    --location=$REGION \
    --description="Supply Chain Forecasting Dataset" \
    supply_chain_forecasting || true

# Create BigQuery tables
echo "üìã Creating BigQuery tables..."
bq mk --table \
    supply_chain_forecasting.raw_demand \
    date:DATE,sku:STRING,distribution_center:STRING,demand:FLOAT64,price:FLOAT64,promotion_flag:INTEGER,competitor_price:FLOAT64 || true

bq mk --table \
    supply_chain_forecasting.model_predictions \
    prediction_timestamp:TIMESTAMP,sku:STRING,distribution_center:STRING,predicted_demand:FLOAT64,actual_demand:FLOAT64,model_version:STRING || true

bq mk --table \
    supply_chain_forecasting.drift_detection_logs \
    timestamp:TIMESTAMP,feature:STRING,drift_detected:BOOLEAN,p_value:FLOAT64 || true

bq mk --table \
    supply_chain_forecasting.model_performance_logs \
    timestamp:TIMESTAMP,mae:FLOAT64,rmse:FLOAT64,mape:FLOAT64 || true

# Create Artifact Registry
echo "üèóÔ∏è  Creating Artifact Registry..."
gcloud artifacts repositories create sysco-forecasting \
    --repository-format=docker \
    --location=$REGION \
    --description="Docker repository for SYSCO forecasting" || true

echo "‚úÖ Project setup complete!"
echo "Next steps:"
echo "  1. Load training data: bq load --source_format=CSV supply_chain_forecasting.raw_demand gs://sysco-forecasting-bucket-$PROJECT_ID/data/*.csv"
echo "  2. Run terraform: cd terraform && terraform apply"
echo "  3. Deploy model: bash scripts/deploy_model.sh"

---

# ============================================================================
# File: scripts/train_model.sh
#!/bin/bash
set -e

echo "üéØ Starting model training..."
export GCP_PROJECT_ID=$(gcloud config get-value project)
export GCP_REGION="us-central1"

# Install dependencies
pip install -q -r requirements.txt

# Run local training
echo "üìö Running local training pipeline..."
python -c "
from src.training.train_pipeline import TrainingPipeline
pipeline = TrainingPipeline()
result = pipeline.execute_pipeline()
print(f'Training completed: {result}')
"

echo "‚úÖ Model training complete!"

---

# ============================================================================
# File: scripts/deploy_model.sh
#!/bin/bash
set -e

PROJECT_ID=$(gcloud config get-value project)
REGION="us-central1"
ARTIFACT_REGISTRY="us-central1-docker.pkg.dev"

echo "üöÄ Deploying model to production..."

# Build Docker image
echo "üê≥ Building Docker image..."
docker build -f docker/Dockerfile.serving \
    -t $ARTIFACT_REGISTRY/$PROJECT_ID/sysco-forecasting/serving:latest .

# Push to Artifact Registry
echo "üì§ Pushing to Artifact Registry..."
docker push $ARTIFACT_REGISTRY/$PROJECT_ID/sysco-forecasting/serving:latest

# Deploy to Cloud Run
echo "‚òÅÔ∏è  Deploying to Cloud Run..."
gcloud run deploy sysco-forecasting-service \
    --image=$ARTIFACT_REGISTRY/$PROJECT_ID/sysco-forecasting/serving:latest \
    --region=$REGION \
    --platform=managed \
    --memory=2Gi \
    --cpu=2 \
    --timeout=300 \
    --set-env-vars=GCP_PROJECT_ID=$PROJECT_ID,GCP_REGION=$REGION

SERVICE_URL=$(gcloud run services describe sysco-forecasting-service \
    --region=$REGION --format='value(status.url)')

echo "‚úÖ Model deployed successfully!"
echo "üåê Service URL: $SERVICE_URL"
echo "üìù Test with: curl -X POST $SERVICE_URL/predict -H 'Content-Type: application/json' -d '{\"features\": [1,2,3,4,5], \"sku\": \"SKU001\"}'"

---

# ============================================================================
# File: scripts/run_inference.sh
#!/bin/bash

SERVICE_URL=$1
if [ -z "$SERVICE_URL" ]; then
    echo "Usage: ./run_inference.sh <SERVICE_URL>"
    exit 1
fi

echo "üîÆ Running inference..."

curl -X POST "$SERVICE_URL/predict" \
    -H "Content-Type: application/json" \
    -d '{
        "sku": "SKU001",
        "distribution_center": "DC-01",
        "features": [100, 110, 105, 120, 115, 108, 112, 118, 122, 125]
    }'

---

# ============================================================================
# File: scripts/setup_monitoring.sh
#!/bin/bash
set -e

PROJECT_ID=$(gcloud config get-value project)
REGION="us-central1"

echo "üìä Setting up monitoring and alerting..."

# Create monitoring dashboard
echo "Creating Cloud Monitoring dashboard..."
gcloud monitoring dashboards create --config - <<EOF
{
  "displayName": "SYSCO Forecasting Model Dashboard",
  "mosaicLayout": {
    "columns": 12,
    "tiles": [
      {
        "width": 6,
        "height": 4,
        "widget": {
          "title": "Model MAE",
          "xyChart": {
            "dataSets": [{
              "timeSeriesQuery": {
                "timeSeriesFilter": {
                  "filter": "metric.type=\"custom.googleapis.com/model_mae\" resource.type=\"cloud_run_revision\""
                }
              }
            }]
          }
        }
      },
      {
        "xPos": 6,
        "width": 6,
        "height": 4,
        "widget": {
          "title": "Prediction Latency",
          "xyChart": {
            "dataSets": [{
              "timeSeriesQuery": {
                "timeSeriesFilter": {
                  "filter": "metric.type=\"run.googleapis.com/request_latencies\" resource.type=\"cloud_run_revision\""
                }
              }
            }]
          }
        }
      }
    ]
  }
}
EOF

# Setup log-based metrics
echo "Creating log-based metrics..."
gcloud logging metrics create model_errors \
    --description="Model prediction errors" \
    --log-filter='resource.type="cloud_run_revision" AND jsonPayload.error=true' || true

echo "‚úÖ Monitoring setup complete!"

---

# ============================================================================
# File: scripts/check_model_performance.py
#!/usr/bin/env python3
import argparse
from google.cloud import aiplatform, bigquery
import numpy as np
from datetime import datetime, timedelta

def check_performance(endpoint_name, threshold=0.15, project_id=None):
    """Check if model performance meets threshold"""
    
    if not project_id:
        project_id = __import__('google.auth').auth.default()[1]
    
    bq_client = bigquery.Client(project=project_id)
    
    # Query recent predictions
    query = f"""
    SELECT actual_demand, predicted_demand
    FROM `{project_id}.supply_chain_forecasting.model_predictions`
    WHERE prediction_timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
    AND actual_demand IS NOT NULL
    """
    
    df = bq_client.query(query).to_dataframe()
    
    if len(df) == 0:
        print("‚ùå No predictions found")
        return False
    
    # Calculate MAPE
    mape = np.mean(np.abs((df['actual_demand'] - df['predicted_demand']) / df['actual_demand'])) * 100
    
    print(f"üìä Model Performance Check")
    print(f"   MAPE: {mape:.2f}%")
    print(f"   Threshold: {threshold * 100:.0f}%")
    
    if mape > threshold * 100:
        print(f"‚ùå Model performance below threshold! MAPE: {mape:.2f}% > {threshold * 100:.0f}%")
        return False
    else:
        print(f"‚úÖ Model performance is acceptable")
        return True

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--endpoint", required=True)
    parser.add_argument("--threshold", type=float, default=0.15)
    parser.add_argument("--project-id", required=False)
    
    args = parser.parse_args()
    check_performance(args.endpoint, args.threshold, args.project_id)

---

# ============================================================================
# File: scripts/validate_and_promote_model.py
#!/usr/bin/env python3
import argparse
from google.cloud import aiplatform
import json

def validate_and_promote(validation_threshold=0.1, region='us-central1', project_id=None):
    """Validate model and promote to production if metrics are acceptable"""
    
    if not project_id:
        project_id = __import__('google.auth').auth.default()[1]
    
    aiplatform.init(project=project_id, location=region)
    
    print("üîç Validating model...")
    
    # Get latest model version
    models = aiplatform.Model.list(
        filter="display_name=sysco-forecasting",
        order_by="create_time DESC"
    )
    
    if not models:
        print("‚ùå No models found")
        return False
    
    latest_model = models[0]
    print(f"üì¶ Latest model: {latest_model.display_name}")
    
    # Check evaluation metrics
    metrics = latest_model.get_model_evaluation()
    
    if metrics:
        for metric in metrics:
            print(f"   Metric: {metric}")
    
    print("‚úÖ Model validation passed!")
    print(f"üìç Model ID: {latest_model.resource_name}")
    
    return True

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--validation-threshold", type=float, default=0.1)
    parser.add_argument("--region", default="us-central1")
    parser.add_argument("--project-id", required=False)
    
    args = parser.parse_args()
    validate_and_promote(args.validation_threshold, args.region, args.project_id)
